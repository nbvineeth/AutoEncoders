{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE-Experiment-4:\n",
    "\n",
    "\n",
    "#### Adversarial Autoencoder (Unsupervised training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Variational Autoencoders (VAE) the KL-divergence based regularization term applied on the latent variables  $z$  can be be difficult to evaluate analytically for choice of prior distribution  $p(z)$  that are more complicated than simple distributions like Gaussian. An Adversarial Autoencoder (AAE) circumvents this problem by replacing the KL-divergence term with an adversarial loss which encourages the approximate posterior distribution  $q(z|x)$  to be closer to the prior distribution  $p(z)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment \n",
    "\n",
    "As we did with VAE, let's learn an Adversarial Autoencoder that generates digits similar to the ones in the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Import necessary modules\n",
    "##########################\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "# Set parameters\n",
    "################\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "seed = 10\n",
    "\n",
    "n_classes = 10\n",
    "z_dim = 2\n",
    "X_dim = 784\n",
    "train_batch_size = 100\n",
    "valid_batch_size = train_batch_size\n",
    "N = 1000\n",
    "epochs = 5\n",
    "\n",
    "params = {}\n",
    "params['cuda'] = cuda\n",
    "params['n_classes'] = n_classes\n",
    "params['z_dim'] = z_dim\n",
    "params['X_dim'] = X_dim\n",
    "params['train_batch_size'] = train_batch_size\n",
    "params['valid_batch_size'] = valid_batch_size\n",
    "params['N'] = N\n",
    "params['epochs'] = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# Load data and create Data loaders\n",
    "###################################\n",
    "\n",
    "def load_data(data_path='./data/VAE/'):\n",
    "    print('loading data!')\n",
    "    trainset_labeled = pickle.load(open(data_path + \"train_labeled.p\", \"rb\"))\n",
    "    trainset_unlabeled = pickle.load(open(data_path + \"train_unlabeled.p\", \"rb\"))\n",
    "    # Set -1 as labels for unlabeled data\n",
    "    trainset_unlabeled.train_labels = torch.from_numpy(np.array([-1] * 47000))\n",
    "    validset = pickle.load(open(data_path + \"validation.p\", \"rb\"))\n",
    "\n",
    "    train_labeled_loader = torch.utils.data.DataLoader(trainset_labeled,\n",
    "                                                       batch_size=train_batch_size,\n",
    "                                                       shuffle=True, **kwargs)\n",
    "\n",
    "    train_unlabeled_loader = torch.utils.data.DataLoader(trainset_unlabeled,\n",
    "                                                         batch_size=train_batch_size,\n",
    "                                                         shuffle=True, **kwargs)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(validset, batch_size=valid_batch_size, shuffle=True)\n",
    "\n",
    "    return train_labeled_loader, train_unlabeled_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Define Networks\n",
    "#################\n",
    "\n",
    "# Encoder\n",
    "class Q_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_net, self).__init__()\n",
    "        self.lin1 = nn.Linear(X_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        # Gaussian code (z)\n",
    "        self.lin3gauss = nn.Linear(N, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        xgauss = self.lin3gauss(x)\n",
    "\n",
    "        return xgauss\n",
    "\n",
    "\n",
    "# Decoder\n",
    "class P_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(P_net, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        self.lin3 = nn.Linear(N, X_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return F.sigmoid(x)\n",
    "\n",
    "\n",
    "class D_net_gauss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D_net_gauss, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        self.lin3 = nn.Linear(N, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return F.sigmoid(self.lin3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# Utility functions\n",
    "###################\n",
    "\n",
    "def save_model(model, filename):\n",
    "    print('Best model so far, saving it...')\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "\n",
    "def report_loss(epoch, D_loss_gauss, G_loss, recon_loss):\n",
    "    '''\n",
    "    Print loss\n",
    "    '''\n",
    "    print('Epoch-{}; D_loss_gauss: {:.4}; G_loss: {:.4}; recon_loss: {:.4}'.format(epoch,\n",
    "                                                                                   D_loss_gauss.data[0],\n",
    "                                                                                   G_loss.data[0],\n",
    "                                                                                   recon_loss.data[0]))\n",
    "\n",
    "\n",
    "def create_latent(Q, loader):\n",
    "    '''\n",
    "    Creates the latent representation for the samples in loader\n",
    "    return:\n",
    "        z_values: numpy array with the latent representations\n",
    "        labels: the labels corresponding to the latent representations\n",
    "    '''\n",
    "    Q.eval()\n",
    "    labels = []\n",
    "\n",
    "    for batch_idx, (X, target) in enumerate(loader):\n",
    "\n",
    "        X = X * 0.3081 + 0.1307\n",
    "        # X.resize_(loader.batch_size, X_dim)\n",
    "        X, target = Variable(X), Variable(target)\n",
    "        labels.extend(target.data.tolist())\n",
    "        if cuda:\n",
    "            X, target = X.cuda(), target.cuda()\n",
    "        # Reconstruction phase\n",
    "        z_sample = Q(X)\n",
    "        if batch_idx > 0:\n",
    "            z_values = np.concatenate((z_values, np.array(z_sample.data.tolist())))\n",
    "        else:\n",
    "            z_values = np.array(z_sample.data.tolist())\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return z_values, labels\n",
    "\n",
    "\n",
    "def get_X_batch(data_loader, params, size=None):\n",
    "    if size is None:\n",
    "        size = data_loader.batch_size\n",
    "\n",
    "    data_loader.batch_size = size\n",
    "\n",
    "    for X, target in data_loader:\n",
    "        break\n",
    "\n",
    "    train_batch_size = params['train_batch_size']\n",
    "    X_dim = params['X_dim']\n",
    "    cuda = params['cuda']\n",
    "\n",
    "    X = X * 0.3081 + 0.1307\n",
    "\n",
    "    X = X[:size]\n",
    "    target = target[:size]\n",
    "\n",
    "    X.resize_(size, X_dim)\n",
    "    X, target = Variable(X), Variable(target)\n",
    "\n",
    "    if cuda:\n",
    "        X, target = X.cuda(), target.cuda()\n",
    "\n",
    "    return X, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Training procedure\n",
    "####################\n",
    "\n",
    "def train(P, Q, D_gauss, P_decoder, Q_encoder, Q_generator, D_gauss_solver, data_loader):\n",
    "    '''\n",
    "    Train procedure for one epoch.\n",
    "    '''\n",
    "    TINY = 1e-15\n",
    "    # Set the networks in train mode (apply dropout when needed)\n",
    "    Q.train()\n",
    "    P.train()\n",
    "    D_gauss.train()\n",
    "\n",
    "    # Loop through the labeled and unlabeled dataset getting one batch of samples from each\n",
    "    # The batch size has to be a divisor of the size of the dataset or it will return\n",
    "    # invalid samples\n",
    "    for X, target in data_loader:\n",
    "\n",
    "        # Load batch and normalize samples to be between 0 and 1\n",
    "        X = X * 0.3081 + 0.1307\n",
    "        X.resize_(train_batch_size, X_dim)\n",
    "        X, target = Variable(X), Variable(target)\n",
    "        if cuda:\n",
    "            X, target = X.cuda(), target.cuda()\n",
    "\n",
    "        # Init gradients\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "        D_gauss.zero_grad()\n",
    "\n",
    "\n",
    "        # Reconstruction phase\n",
    "\n",
    "        z_sample = Q(X)\n",
    "        X_sample = P(z_sample)\n",
    "        recon_loss = F.binary_cross_entropy(X_sample + TINY, X.resize(train_batch_size, X_dim) + TINY)\n",
    "\n",
    "        recon_loss.backward()\n",
    "        P_decoder.step()\n",
    "        Q_encoder.step()\n",
    "\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "        D_gauss.zero_grad()\n",
    "\n",
    "\n",
    "        # Regularization phase\n",
    "\n",
    "        # Discriminator\n",
    "        Q.eval()\n",
    "        z_real_gauss = Variable(torch.randn(train_batch_size, z_dim) * 5.)\n",
    "        if cuda:\n",
    "            z_real_gauss = z_real_gauss.cuda()\n",
    "\n",
    "        z_fake_gauss = Q(X)\n",
    "\n",
    "        D_real_gauss = D_gauss(z_real_gauss)\n",
    "        D_fake_gauss = D_gauss(z_fake_gauss)\n",
    "\n",
    "        D_loss = -torch.mean(torch.log(D_real_gauss + TINY) + torch.log(1 - D_fake_gauss + TINY))\n",
    "\n",
    "        D_loss.backward()\n",
    "        D_gauss_solver.step()\n",
    "\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "        D_gauss.zero_grad()\n",
    "\n",
    "        # Generator\n",
    "        Q.train()\n",
    "        z_fake_gauss = Q(X)\n",
    "\n",
    "        D_fake_gauss = D_gauss(z_fake_gauss)\n",
    "        G_loss = -torch.mean(torch.log(D_fake_gauss + TINY))\n",
    "\n",
    "        G_loss.backward()\n",
    "        Q_generator.step()\n",
    "\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "        D_gauss.zero_grad()\n",
    "\n",
    "    return D_loss, G_loss, recon_loss\n",
    "\n",
    "\n",
    "def generate_model(train_labeled_loader, train_unlabeled_loader, valid_loader):\n",
    "    torch.manual_seed(10)\n",
    "\n",
    "    if cuda:\n",
    "        Q = Q_net().cuda()\n",
    "        P = P_net().cuda()\n",
    "        D_gauss = D_net_gauss().cuda()\n",
    "    else:\n",
    "        Q = Q_net()\n",
    "        P = P_net()\n",
    "        D_gauss = D_net_gauss()\n",
    "\n",
    "    # Set learning rates\n",
    "    gen_lr = 0.0001\n",
    "    reg_lr = 0.00005\n",
    "\n",
    "    # Set optimizators\n",
    "    P_decoder = optim.Adam(P.parameters(), lr=gen_lr)\n",
    "    Q_encoder = optim.Adam(Q.parameters(), lr=gen_lr)\n",
    "\n",
    "    Q_generator = optim.Adam(Q.parameters(), lr=reg_lr)\n",
    "    D_gauss_solver = optim.Adam(D_gauss.parameters(), lr=reg_lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        D_loss_gauss, G_loss, recon_loss = train(P, Q, D_gauss, P_decoder, Q_encoder,\n",
    "                                                 Q_generator,\n",
    "                                                 D_gauss_solver,\n",
    "                                                 train_unlabeled_loader)\n",
    "        if epoch % 1 == 0:\n",
    "            report_loss(epoch, D_loss_gauss, G_loss, recon_loss)\n",
    "\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Train a generative model\n",
    "##########################\n",
    "\n",
    "train_labeled_loader, train_unlabeled_loader, valid_loader = load_data()\n",
    "Q, P = generate_model(train_labeled_loader, train_unlabeled_loader, valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Save trained model\n",
    "####################\n",
    "\n",
    "# Save trained model\n",
    "# torch.save(Q,'TrainedModels/AAE_mytraining_Q.pt')\n",
    "# torch.save(P,'TrainedModels/AAE_mytraining_P.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Load trained model\n",
    "####################\n",
    "\n",
    "# Load model trained for 500 epochs\n",
    "Q_pt = torch.load('./data/VAE/TrainedModels/AAE_preTrained_Q.pt')\n",
    "P_pt = torch.load('./data/VAE/TrainedModels/AAE_preTrained_P.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Visualize reconstruction\n",
    "##########################\n",
    "\n",
    "def create_reconstruction(Q, P, data_loader, params):\n",
    "    Q.eval()\n",
    "    P.eval()\n",
    "    X, label = get_X_batch(data_loader, params, size=1)\n",
    "\n",
    "    z = Q(X)\n",
    "    x = P(z)\n",
    "\n",
    "    img_orig = np.array(X[0].data.tolist()).reshape(28, 28)\n",
    "    img_rec = np.array(x[0].data.tolist()).reshape(28, 28)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_orig)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img_rec)\n",
    "\n",
    "\n",
    "data_loader = valid_loader    # Training data:  train_unlabeled_loader  |  Validation data:  valid_loader\n",
    "\n",
    "create_reconstruction(Q_pt, P_pt, data_loader, params)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Visualize generation\n",
    "######################\n",
    "\n",
    "def grid_plot2d(Q, P, data_loader, params):\n",
    "    Q.eval()\n",
    "    P.eval()\n",
    "\n",
    "    cuda = params['cuda']\n",
    "\n",
    "    z1 = Variable(torch.from_numpy(np.arange(-10, 10, 1.5).astype('float32')))\n",
    "    z2 = Variable(torch.from_numpy(np.arange(-10, 10, 1.5).astype('float32')))\n",
    "    if cuda:\n",
    "        z1, z2 = z1.cuda(), z2.cuda()\n",
    "\n",
    "    nx, ny = len(z1), len(z2)\n",
    "    plt.subplot()\n",
    "    gs = gridspec.GridSpec(nx, ny, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i, g in enumerate(gs):\n",
    "        z = torch.cat((z1[i / ny], z2[i % nx])).resize(1, 2)\n",
    "        x = P(z)\n",
    "\n",
    "        ax = plt.subplot(g)\n",
    "        img = np.array(x.data.tolist()).reshape(28, 28)\n",
    "        ax.imshow(img, )\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_aspect('auto')\n",
    "\n",
    "\n",
    "grid_plot2d(Q_pt, P_pt, data_loader, params)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# Visualize latent distribution\n",
    "###############################\n",
    "\n",
    "Q_pt.eval()\n",
    "P_pt.eval()\n",
    "\n",
    "X, label = get_X_batch(data_loader, params, size=10000)\n",
    "z = Q_pt(X)\n",
    "\n",
    "z = np.array(z.data.tolist())\n",
    "label = np.array(label.data.tolist())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(z[:,0], z[:,1], c=label, cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare the latent distribution as learned by the Adversarial Autoencoder to that of the Variational Autoencoder. Do you feel any one of the representation is clearly better than the other?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
